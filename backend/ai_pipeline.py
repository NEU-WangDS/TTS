# æª”æ¡ˆä½ç½®: backend/ai_pipeline.py

import torch
import os
import sys

PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.realpath(__file__)))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

import jiwer
import soundfile as sf
import numpy as np
from tqdm import tqdm
import jieba
from opencc import OpenCC
# -------------------------------------------------------------------
# å°å…¥æ‰€æœ‰éœ€è¦çš„å‡½å¼åº«
# -------------------------------------------------------------------

# ç¿»è­¯æ¨¡å‹ (NLLB)
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer

# èªéŸ³è¾¨è­˜æ¨¡å‹ (Whisper)
from transformers import AutoModelForSpeechSeq2Seq, AutoProcessor, pipeline

# èªéŸ³åˆæˆæ¨¡å‹ (MaskGCT from Amphion)
# ç”±æ–¼ models, third_party, utils è³‡æ–™å¤¾ç¾åœ¨ä½æ–¼ backend å…§éƒ¨ï¼Œ
# Python å¯ä»¥ç›´æ¥æ‰¾åˆ°ä¸¦å°å…¥å®ƒå€‘ï¼Œç„¡éœ€ä»»ä½•è·¯å¾‘æ“ä½œã€‚
from models.tts.maskgct.maskgct_utils import (
    build_semantic_model,
    build_semantic_codec,
    build_acoustic_codec,
    build_t2s_model,
    build_s2a_model,
    load_config,
    MaskGCT_Inference_Pipeline,
)
from huggingface_hub import hf_hub_download
import safetensors


class AIPipeline:
    """
    ä¸€å€‹å°è£äº†æ‰€æœ‰ AI æ¨¡å‹ï¼ˆç¿»è­¯ã€TTSã€ASRï¼‰çš„ç®¡ç·šé¡åˆ¥ã€‚
    æ¨¡å‹åªåœ¨ä¼ºæœå™¨å•Ÿå‹•æ™‚è¼‰å…¥ä¸€æ¬¡ã€‚
    """
    def __init__(self):
        print("="*50)
        print("æ­£åœ¨åˆå§‹åŒ– AI ç®¡ç·šï¼Œé–‹å§‹è¼‰å…¥æ‰€æœ‰æ¨¡å‹...")
        print("="*50)
        
        self.device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
        self.torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32
        self.cc = OpenCC('t2s')
        # --- 1. è¼‰å…¥ NLLB ç¿»è­¯æ¨¡å‹ ---
        self._load_translator_model()

        # --- 2. è¼‰å…¥ MaskGCT TTS æ¨¡å‹ ---
        self._load_tts_model()

        # --- 3. è¼‰å…¥ Whisper ASR æ¨¡å‹ ---
        self._load_asr_model()
        
        print("\nâœ… æ‰€æœ‰ AI æ¨¡å‹æº–å‚™å°±ç·’ï¼ä¼ºæœå™¨å¯ä»¥é–‹å§‹æ¥æ”¶è«‹æ±‚ã€‚")

    def _load_translator_model(self):
        print("\n[1/3] æ­£åœ¨è¼‰å…¥ NLLB ç¿»è­¯æ¨¡å‹...")
        model_id = "facebook/nllb-200-distilled-600M"
        self.translator_model = AutoModelForSeq2SeqLM.from_pretrained(model_id).to(self.device)
        self.translator_tokenizer = AutoTokenizer.from_pretrained(model_id)
        print("NLLB ç¿»è­¯æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")

    def _load_tts_model(self):
        print("\n[2/3] æ­£åœ¨è¼‰å…¥ MaskGCT TTS æ¨¡å‹...")
        # ä½¿ç”¨æœ¬åœ°ç›¸å°è·¯å¾‘ä¾†è®€å–è¨­å®šæª”
        cfg_path = "./models/tts/maskgct/config/maskgct.json"
        cfg = load_config(cfg_path)
        
        # ğŸ’¡ é—œéµä¿®æ”¹è™•ï¼šå°‡ semantic_model çš„è¨­å®šå‚³å…¥ build_semantic_model å‡½å¼
        # é€™æœƒè®“å®ƒå¾æœ¬åœ° ckpt è¼‰å…¥ï¼Œè€Œä¸æ˜¯å¾ transformers ä¸‹è¼‰
        # semantic_model, semantic_mean, semantic_std = build_semantic_model(
        #     cfg.model.semantic_model, self.device
        # )
        semantic_model, semantic_mean, semantic_std = build_semantic_model(self.device)
        semantic_codec = build_semantic_codec(cfg.model.semantic_codec, self.device)
        codec_encoder, codec_decoder = build_acoustic_codec(cfg.model.acoustic_codec, self.device)
        t2s_model = build_t2s_model(cfg.model.t2s_model, self.device)
        s2a_model_1layer = build_s2a_model(cfg.model.s2a_model.s2a_1layer, self.device)
        s2a_model_full = build_s2a_model(cfg.model.s2a_model.s2a_full, self.device)

        print("    æ­£åœ¨å¾ Hugging Face ä¸‹è¼‰ MaskGCT æ¨¡å‹æ¬Šé‡...")
        semantic_code_ckpt = hf_hub_download("amphion/MaskGCT", filename="semantic_codec/model.safetensors")
        codec_encoder_ckpt = hf_hub_download("amphion/MaskGCT", filename="acoustic_codec/model.safetensors")
        codec_decoder_ckpt = hf_hub_download("amphion/MaskGCT", filename="acoustic_codec/model_1.safetensors")
        t2s_model_ckpt = hf_hub_download("amphion/MaskGCT", filename="t2s_model/model.safetensors")
        s2a_1layer_ckpt = hf_hub_download("amphion/MaskGCT", filename="s2a_model/s2a_model_1layer/model.safetensors")
        s2a_full_ckpt = hf_hub_download("amphion/MaskGCT", filename="s2a_model/s2a_model_full/model.safetensors")

        safetensors.torch.load_model(semantic_codec, semantic_code_ckpt)
        safetensors.torch.load_model(codec_encoder, codec_encoder_ckpt)
        safetensors.torch.load_model(codec_decoder, codec_decoder_ckpt)
        safetensors.torch.load_model(t2s_model, t2s_model_ckpt)
        safetensors.torch.load_model(s2a_model_1layer, s2a_1layer_ckpt)
        safetensors.torch.load_model(s2a_model_full, s2a_full_ckpt)
        
        self.maskgct_pipeline = MaskGCT_Inference_Pipeline(
            semantic_model, semantic_codec, codec_encoder, codec_decoder, t2s_model,
            s2a_model_1layer, s2a_model_full, semantic_mean, semantic_std, self.device
        )
        print("MaskGCT TTS æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")

    def _load_asr_model(self):
        print("\n[3/3] æ­£åœ¨è¼‰å…¥ Whisper ASR æ¨¡å‹...")
        model_id = "openai/whisper-large-v3"
        model = AutoModelForSpeechSeq2Seq.from_pretrained(
            model_id, torch_dtype=self.torch_dtype, low_cpu_mem_usage=True, use_safetensors=True
        )
        model.to(self.device)
        processor = AutoProcessor.from_pretrained(model_id)
        
        self.whisper_pipe = pipeline(
            "automatic-speech-recognition",
            model=model,
            tokenizer=processor.tokenizer,
            feature_extractor=processor.feature_extractor,
            torch_dtype=self.torch_dtype,
            device=self.device,
        )
        print("Whisper ASR æ¨¡å‹è¼‰å…¥å®Œæˆã€‚")

    def translate(self, text: str, target_lang_code: str, src_lang_code: str = "eng_Latn") -> str:
        """
        ä½¿ç”¨ NLLB æ¨¡å‹ç¿»è­¯æ–‡æœ¬ã€‚
        """
        print(f"åŸ·è¡Œç¿»è­¯: {text[:20]}... -> {target_lang_code}")
        self.translator_tokenizer.src_lang = src_lang_code
        inputs = self.translator_tokenizer(text, return_tensors="pt").to(self.device)
        translated_tokens = self.translator_model.generate(
            **inputs, forced_bos_token_id=self.translator_tokenizer.lang_code_to_id[target_lang_code]
        )
        translated_text = self.translator_tokenizer.batch_decode(translated_tokens, skip_special_tokens=True)[0]
        return translated_text

    def synthesize(self, text: str, lang: str, prompt_wav_path: str) -> (np.ndarray, int):
        """
        ä½¿ç”¨ MaskGCT æ¨¡å‹åˆæˆèªéŸ³ã€‚
        è¿”å› (éŸ³è¨Š NumPy é™£åˆ—, æ¡æ¨£ç‡)ã€‚
        """
        print(f"åŸ·è¡ŒèªéŸ³åˆæˆ: {text[:20]}... (èªè¨€: {lang})")
        
        # é—œéµä¿®æ”¹ï¼šå‹•æ…‹è­˜åˆ¥ prompt éŸ³è¨Šçš„å…§å®¹ï¼Œä½¿å…¶æ›´ç©©å¥
        print(f"    æ­£åœ¨è­˜åˆ¥éŸ³è‰²æ¨£æœ¬ '{os.path.basename(prompt_wav_path)}' çš„å…§å®¹...")
        prompt_lang = "en" # å‡è¨­æ‰€æœ‰éŸ³è‰²æ¨£æœ¬éƒ½æ˜¯è‹±æ–‡
        prompt_text = self.recognize(prompt_wav_path, prompt_lang)
        print(f"    è­˜åˆ¥å‡ºçš„éŸ³è‰²æ¨£æœ¬å…§å®¹: '{prompt_text[:30]}...'")
        
        with torch.cuda.amp.autocast():
            recovered_audio = self.maskgct_pipeline.maskgct_inference(
                prompt_wav_path, prompt_text, text, prompt_lang, lang, target_len=None
            )
        return recovered_audio, 24000

    def recognize(self, audio_path: str, lang: str) -> str:
            """
            ä½¿ç”¨ Whisper æ¨¡å‹è¯†åˆ«è¯­éŸ³ã€‚
            (æœ€ç»ˆç‰ˆï¼šåœ¨è¾“å‡ºä¸­æ–‡ç»“æœåï¼Œç«‹å³ç»Ÿä¸€ä¸ºç®€ä½“)
            """
            print(f"æ‰§è¡Œè¯­éŸ³è¯†åˆ«: {audio_path} (è¯­è¨€: {lang})")
            
            # è°ƒç”¨ Whisper æ¨¡å‹
            result = self.whisper_pipe(
                audio_path,
                generate_kwargs={"language": lang, "task": "transcribe"}
            )
            recognized_text = result.get("text", "").strip()

            # æ ¸å¿ƒé€»è¾‘ï¼šå¦‚æœæ˜¯ä¸­æ–‡ï¼Œç«‹å³è¿›è¡Œç®€ç¹ç»Ÿä¸€
            if lang == 'chinese':
                print("      æ£€æµ‹åˆ°ä¸­æ–‡è¯†åˆ«ç»“æœï¼Œæ‰§è¡Œç®€ç¹ç»Ÿä¸€...")
                try:
                    simplified_text = self.cc.convert(recognized_text)
                    return simplified_text
                except Exception as e:
                    print(f"      [è­¦å‘Š] OpenCC ç®€ç¹è½¬æ¢å¤±è´¥: {e}")
                    # å³ä½¿è½¬æ¢å¤±è´¥ï¼Œä¹Ÿè¿”å›åŸå§‹è¯†åˆ«ç»“æœ
                    return recognized_text
            
            # å¦‚æœä¸æ˜¯ä¸­æ–‡ï¼Œç›´æ¥è¿”å›åŸå§‹è¯†åˆ«ç»“æœ
            return recognized_text
    


    # åœ¨ backend/ai_pipeline.py æ–‡ä»¶ä¸­ï¼Œæ‰¾åˆ°å¹¶æ›¿æ¢è¿™ä¸ªå‡½æ•°

    def evaluate(self, reference_text: str, hypothesis_text: str, lang_code: str) -> dict:
        """
        ä½¿ç”¨ Jiwer è®¡ç®— WER å’Œ CERã€‚
        (æœ€ç»ˆç‰ˆï¼šé‡‡ç”¨æ ‡å‡†èŒƒå¼å¤„ç†ä¸­æ–‡åˆ†è¯ï¼Œä»¥è§£å†³ jiwer çš„è§£æé”™è¯¯)
        """
        print(f"æ‰§è¡Œè¯„ä¼° (è¯­è¨€: {lang_code})...")
        
        # 1. å®šä¹‰é€šç”¨çš„æ–‡æœ¬æ ‡å‡†åŒ–è§„åˆ™
        transformation = jiwer.Compose([
            jiwer.ToLowerCase(),
            jiwer.RemoveMultipleSpaces(),
            jiwer.Strip(),
            jiwer.RemovePunctuation(),
        ])
        
        # 2. å¯¹ä¸¤ä¸ªå­—ç¬¦ä¸²åº”ç”¨æ ‡å‡†åŒ–è§„åˆ™
        processed_reference = transformation(reference_text)
        processed_hypothesis = transformation(hypothesis_text)

        # 3. æ™ºèƒ½è®¡ç®— WER (æœ€ç»ˆä¿®æ­£ç‰ˆ)
        if lang_code == 'zh':
            print("      æ£€æµ‹åˆ°ä¸­æ–‡ï¼Œä½¿ç”¨ jieba åˆ†è¯å¹¶ç”¨ç©ºæ ¼è¿æ¥...")
            # å¯¹äºä¸­æ–‡ï¼Œå…ˆç”¨ jieba åˆ†è¯ï¼Œç„¶åç”¨ç©ºæ ¼å°†è¯è¯­è¿æ¥æˆä¸€ä¸ªæ ‡å‡†å­—ç¬¦ä¸²
            ref_words = " ".join(jieba.lcut(processed_reference))
            hyp_words = " ".join(jieba.lcut(processed_hypothesis))
            
            # å°†ä¸¤ä¸ªå¤„ç†å¥½çš„ã€ç©ºæ ¼åˆ†éš”çš„å­—ç¬¦ä¸²äº¤ç»™ jiwer
            wer_score = jiwer.wer(ref_words, hyp_words)
        else:
            # å¯¹äºå…¶ä»–è¯­è¨€ï¼Œç›´æ¥ä½¿ç”¨ jiwer é»˜è®¤çš„ç©ºæ ¼åˆ†è¯
            wer_score = jiwer.wer(processed_reference, processed_hypothesis)
            
        # 4. CER æ˜¯åŸºäºå­—ç¬¦çš„ï¼Œä¸å—åˆ†è¯å½±å“ï¼Œå¯ä»¥ç›´æ¥è®¡ç®—
        # æˆ‘ä»¬ä»ç„¶ä½¿ç”¨å¤„ç†è¿‡çš„å­—ç¬¦ä¸²ï¼Œä»¥ç¡®ä¿å…¬å¹³æ€§ (ç§»é™¤äº†æ ‡ç‚¹å’Œå¤šä½™ç©ºæ ¼)
        cer_score = jiwer.cer(processed_reference, processed_hypothesis)

        return {
            "wer": wer_score,
            "cer": cer_score,
        }
# --- å…¨å±€å¯¦ä¾‹ ---
# åœ¨æ‡‰ç”¨ç¨‹å¼å•Ÿå‹•æ™‚ï¼Œåªå»ºç«‹ä¸€æ¬¡ AIPipeline çš„å¯¦ä¾‹ã€‚
# FastAPI ä¸»ç¨‹å¼ (main.py) å°‡æœƒå°å…¥ä¸¦ä½¿ç”¨é€™å€‹ç‰©ä»¶ã€‚
ai_pipeline = AIPipeline()
